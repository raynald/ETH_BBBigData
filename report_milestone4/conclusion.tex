\section{Conclusion}

Althgouh the result of our project is quite subjective, there is an optimal number of clusters. After enough iterations, the result remains a good quality. For the ones with less number of clusters, it would take less time to compute while for bigger number of clusters, it would take more time. From \cite{sklearn}'s website, there is a note as following
\begin{quote}
The k-means problem is solved using Lloyd’s algorithm.

The average complexity is given by $O(k n T)$, were n is the number of samples and T is the number of iteration.

The worst case complexity is given by $O(n^{(k+2/p)})$ with n = n\_samples, p = n\_features. (D. Arthur and S. Vassilvitskii, `How slow is the k-means method?' SoCG2006)

In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.
\end{quote}

From Section {\color{red}Introduction}, we can see that the size of data varies a lot. The more data we use, the more time it would take to run. 

\subsection{Difficulties}
\begin{itemize}
    \item The size of Dataset is too big to fit in the memory.
    \item Too inefficient to run K-means on the whole dataset
    \item Install Python tools in the remote machine
\end{itemize}

\subsection{Lecture Learned}
\begin{itemize}
    \item Debug thoroughly before implementing it in a distributed system
    \item Saved the log whenever necessary
\end{itemize}

\section{Future Work}
\begin{itemize}
    \item Create more features and do feature selection
    \item Other clustering algorithm, maybe GMM 
\end{itemize}

