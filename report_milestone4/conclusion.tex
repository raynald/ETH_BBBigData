\section{Conclusion}

Althgouh the result of our project is quite subjective, there is an optimal number of clusters. After enough iterations, the result remains a good quality. For the ones with less number of clusters, it would take less time to compute while for bigger number of clusters, it would take more time. Because from \cite{sklearn}'s website, there is a note as following
\begin{quote}
The k-means problem is solved using Lloyd’s algorithm.

The average complexity is given by $O(k n T)$, were n is the number of samples and T is the number of iteration.

The worst case complexity is given by $O(n^{(k+2/p)})$ with n = n\_samples, p = n\_features. (D. Arthur and S. Vassilvitskii, `How slow is the k-means method?' SoCG2006)

In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.
\end{quote}

From Section {\color{red}Introduction}, we can see that the size of data varies a lot. The more data we use, the more time it would take to run. 

\subsection{Difficulties}

\subsection{Lecture Learned}

\section{Future Work}

