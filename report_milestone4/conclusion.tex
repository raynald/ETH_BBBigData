\section{Conclusion}

Althgouh the result of our project is quite subjective, there is an optimal number of clusters. After enough iterations, the result remains a good quality. For the ones with less number of clusters, it would take less time to compute while for bigger number of clusters, it would take more time. From \cite{sklearn}'s website, there is a note for the complexity of algorithm:
\begin{quote}
The k-means problem is solved using Lloyd’s algorithm.

The average complexity is given by $O(k n T)$, were n is the number of samples and T is the number of iteration.

The worst case complexity is given by $O(n^{(k+2/p)})$ with n = n\_samples, p = n\_features. (D. Arthur and S. Vassilvitskii, `How slow is the k-means method?' SoCG2006)

In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.
\end{quote}
Where in our experiment, we choose $150$ clusters and $500$ iterations. The solution quality would improve if we use more `useful' data. We throw away the station which have huge number of missing values. The more `useful' data we use, the more computational effort would take. So that's the trade-off.

\subsection{Difficulties}
We encountered several difficulties during the projects. The top major ones is following:
\begin{itemize}
    \item The size of Dataset is too big to fit in the memory;
    \item Too in-efficient to run K-means on the whole dataset;
    \item Install Python tools in the remote machine.
\end{itemize}

\subsection{Lecture Learned}
From this project, we learned quite a lot lessons, which could be improved when we do future big data project:
\begin{itemize}
    \item Debug thoroughly before implementing it in a distributed system;
    \item Save the log whenever necessary.
\end{itemize}

\section{Future Work}
In future, we have two directions to go to improve our system: one is related with features, the other is about the clustering algorithm:
\begin{itemize}
    \item Create more features and do feature selection;
    \item Other clustering algorithms, maybe GMM.
\end{itemize}
